{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises \n",
    "\n",
    "## 0. Setup your own repo\n",
    "- Dont work in this repo. This is my material for lessons. Set up your own repo to work in. Use `MADS-ML-{yourname}` as a format, eg `MADS-ML-JoostB`.\n",
    "- You can add `mads_datasets` and `mltrainer` as dependencies to your own repo, in addition to more basic things like jupyter, torch and seaborn.\n",
    "- If you want to use the tomlserializer, add `tomlserializer` as a dependency. For tensorboard, add `tensorboard` and `torch-tb-profiler`.\n",
    "- Invite me (raoulg; https://github.com/raoulg) as a collaborator to your repo.\n",
    "\n",
    "# 1. Tune the network\n",
    "Run the experiment below, explore the different parameters (see suggestions below) and study the result with tensorboard. \n",
    "Make a single page (1 a4) report of your findings. Use your visualisation skills to communicate your most important findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import imagemodels, Trainer, TrainerSettings, ReportTypes, metrics\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tomlserializer import TOMLSerializer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `tomlserializer` to easily keep track of our experiments, and to easily save the different things we did during our experiments.\n",
    "It can export things like settings and models to a simple `toml` file, which can be easily shared, checked and modified.\n",
    "\n",
    "First, we need the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 19:40:52.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /Users/stevenbontius/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2026-02-16 19:40:52.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /Users/stevenbontius/.cache/mads_datasets/fashionmnist/fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "preprocessor = BasePreprocessor()\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to determine how well our model is performing. We will use accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set up a single experiment.\n",
    "\n",
    "- We will show the model batches of 64 images, \n",
    "- and for every epoch we will show the model 100 batches (trainsteps=100).\n",
    "- then, we will test how well the model is doing on unseen data (teststeps=100).\n",
    "- we will report our results during training to tensorboard, and report all configuration to a toml file.\n",
    "- we will log the results into a directory called \"modellogs\", but you could change this to whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=3,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a very basic model: a model with three linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    num_classes=10, units1=256, units2=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I developped the `tomlserializer` package, it is a useful tool to save configs, models and settings as a tomlfile; that way it is easy to track what you changed during your experiments.\n",
    "\n",
    "This package will 1. check if there is a `__dict__` attribute available, and if so, it will use that to extract the parameters that do not start with an underscore, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True, 'num_classes': 10, 'units1': 256, 'units2': 256}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in model.__dict__.items() if not k.startswith(\"_\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if you want to add more parameters to the `.toml` file, eg `units3`, you can add them to the class like this:\n",
    "\n",
    "```python\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int, units3: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.units3 = units3  # <-- add this line\n",
    "```\n",
    "\n",
    "And then it will be added to the `.toml` file. Check the result for yourself by using the `.save()` method of the `TomlSerializer` class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomlserializer = TOMLSerializer()\n",
    "tomlserializer.save(settings, \"settings.toml\")\n",
    "tomlserializer.save(model, \"model.toml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `settings.toml` and `model.toml` files to see what is in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `Trainer` class from my `mltrainer` module to train your model. It has the TOMLserializer integrated, so it will automatically save the settings and model to a toml file if you have added `TOML` as a reporttype in the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 19:40:52.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/20260216-194052\u001b[0m\n",
      "\u001b[32m2026-02-16 19:40:52.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:00<00:00, 871.88it/s]\n",
      "\u001b[32m2026-02-16 19:40:52.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.9215 test 0.6879 metric ['0.7391']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:00<00:00, 894.41it/s]\n",
      "\u001b[32m2026-02-16 19:40:52.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.5824 test 0.5872 metric ['0.7863']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:00<00:00, 838.50it/s]\n",
      "\u001b[32m2026-02-16 19:40:53.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5023 test 0.5073 metric ['0.8147']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:00<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    ")\n",
    "trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check in the modellogs directory the results of your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop this with a naive approach, called a grid-search (why do you think i call it naive?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units: 256, 256\n",
      "Units: 256, 128\n",
      "Units: 256, 64\n",
      "Units: 128, 256\n",
      "Units: 128, 128\n",
      "Units: 128, 64\n",
      "Units: 64, 256\n",
      "Units: 64, 128\n",
      "Units: 64, 64\n"
     ]
    }
   ],
   "source": [
    "units = [256, 128, 64]\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "        print(f\"Units: {unit1}, {unit2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this might not be the best way to search for a model; some configurations will be better than others (can you predict up front what will be the best configuration?).\n",
    "\n",
    "So, feel free to improve upon the gridsearch by adding your own logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-17 18:16:24.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_256_256/20260217-181624\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:24.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 813.79it/s]\n",
      "\u001b[32m2026-02-17 18:16:25.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5091 test 0.4339 metric ['0.8343']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 944.11it/s]\n",
      "\u001b[32m2026-02-17 18:16:26.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3672 test 0.4068 metric ['0.8528']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 937.46it/s]\n",
      "\u001b[32m2026-02-17 18:16:27.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3281 test 0.3533 metric ['0.8689']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 917.82it/s]\n",
      "\u001b[32m2026-02-17 18:16:28.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3073 test 0.3596 metric ['0.8735']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:28.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3533, current loss 0.3596.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 931.93it/s]\n",
      "\u001b[32m2026-02-17 18:16:29.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2851 test 0.3271 metric ['0.8819']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 680.89it/s]\n",
      "\u001b[32m2026-02-17 18:16:31.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2727 test 0.3331 metric ['0.8860']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:31.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3271, current loss 0.3331.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 920.92it/s]\n",
      "\u001b[32m2026-02-17 18:16:32.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2574 test 0.3284 metric ['0.8873']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:32.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3271, current loss 0.3284.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 836.56it/s]\n",
      "\u001b[32m2026-02-17 18:16:33.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2448 test 0.3333 metric ['0.8812']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:33.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3271, current loss 0.3333.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 868.77it/s]\n",
      "\u001b[32m2026-02-17 18:16:34.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2354 test 0.3194 metric ['0.8898']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 846.15it/s]\n",
      "\u001b[32m2026-02-17 18:16:35.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2278 test 0.3282 metric ['0.8820']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:35.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3194, current loss 0.3282.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:11<00:00,  1.17s/it]\n",
      "\u001b[32m2026-02-17 18:16:35.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_256_128/20260217-181635\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:35.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 964.07it/s]\n",
      "\u001b[32m2026-02-17 18:16:36.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5192 test 0.4256 metric ['0.8492']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1039.97it/s]\n",
      "\u001b[32m2026-02-17 18:16:37.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3712 test 0.3829 metric ['0.8599']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1053.21it/s]\n",
      "\u001b[32m2026-02-17 18:16:38.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3315 test 0.3617 metric ['0.8716']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1022.55it/s]\n",
      "\u001b[32m2026-02-17 18:16:39.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3065 test 0.3579 metric ['0.8775']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1055.72it/s]\n",
      "\u001b[32m2026-02-17 18:16:40.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2893 test 0.3667 metric ['0.8712']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:40.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3579, current loss 0.3667.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1063.56it/s]\n",
      "\u001b[32m2026-02-17 18:16:41.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2714 test 0.3289 metric ['0.8835']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1024.48it/s]\n",
      "\u001b[32m2026-02-17 18:16:42.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2635 test 0.3363 metric ['0.8807']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:42.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3289, current loss 0.3363.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 937.82it/s] \n",
      "\u001b[32m2026-02-17 18:16:43.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2507 test 0.3401 metric ['0.8777']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:43.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3289, current loss 0.3401.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1066.28it/s]\n",
      "\u001b[32m2026-02-17 18:16:44.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2378 test 0.3534 metric ['0.8852']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:44.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3289, current loss 0.3534.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1085.25it/s]\n",
      "\u001b[32m2026-02-17 18:16:45.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2320 test 0.3315 metric ['0.8841']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:45.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3289, current loss 0.3315.Counter 4/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:09<00:00,  1.01it/s]\n",
      "\u001b[32m2026-02-17 18:16:45.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_256_64/20260217-181645\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:45.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 905.53it/s]\n",
      "\u001b[32m2026-02-17 18:16:46.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5406 test 0.4843 metric ['0.8224']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1017.71it/s]\n",
      "\u001b[32m2026-02-17 18:16:47.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3791 test 0.3888 metric ['0.8602']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1058.07it/s]\n",
      "\u001b[32m2026-02-17 18:16:48.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3353 test 0.3631 metric ['0.8691']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 807.10it/s]\n",
      "\u001b[32m2026-02-17 18:16:50.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3114 test 0.3498 metric ['0.8749']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1044.65it/s]\n",
      "\u001b[32m2026-02-17 18:16:51.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2978 test 0.3381 metric ['0.8827']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1034.06it/s]\n",
      "\u001b[32m2026-02-17 18:16:52.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2812 test 0.3474 metric ['0.8754']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:52.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3381, current loss 0.3474.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1066.19it/s]\n",
      "\u001b[32m2026-02-17 18:16:53.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2673 test 0.3351 metric ['0.8802']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1090.33it/s]\n",
      "\u001b[32m2026-02-17 18:16:53.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2606 test 0.3265 metric ['0.8858']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1121.19it/s]\n",
      "\u001b[32m2026-02-17 18:16:54.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2477 test 0.3369 metric ['0.8845']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:54.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3265, current loss 0.3369.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 996.86it/s]\n",
      "\u001b[32m2026-02-17 18:16:55.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2399 test 0.3319 metric ['0.8841']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:55.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3265, current loss 0.3319.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:10<00:00,  1.02s/it]\n",
      "\u001b[32m2026-02-17 18:16:55.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_128_256/20260217-181655\u001b[0m\n",
      "\u001b[32m2026-02-17 18:16:55.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1089.18it/s]\n",
      "\u001b[32m2026-02-17 18:16:56.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5278 test 0.4209 metric ['0.8460']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1066.75it/s]\n",
      "\u001b[32m2026-02-17 18:16:57.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3817 test 0.4069 metric ['0.8517']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 979.70it/s] \n",
      "\u001b[32m2026-02-17 18:16:58.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3375 test 0.3790 metric ['0.8627']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1106.66it/s]\n",
      "\u001b[32m2026-02-17 18:16:59.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3179 test 0.3550 metric ['0.8734']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1054.08it/s]\n",
      "\u001b[32m2026-02-17 18:17:00.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2911 test 0.3517 metric ['0.8723']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1002.15it/s]\n",
      "\u001b[32m2026-02-17 18:17:01.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2811 test 0.3518 metric ['0.8724']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:01.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3517, current loss 0.3518.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1074.77it/s]\n",
      "\u001b[32m2026-02-17 18:17:02.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2727 test 0.3366 metric ['0.8803']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1113.48it/s]\n",
      "\u001b[32m2026-02-17 18:17:03.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2567 test 0.3336 metric ['0.8803']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1026.93it/s]\n",
      "\u001b[32m2026-02-17 18:17:04.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2470 test 0.3348 metric ['0.8789']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:04.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3336, current loss 0.3348.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 952.82it/s]\n",
      "\u001b[32m2026-02-17 18:17:05.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2374 test 0.3438 metric ['0.8780']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:05.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3336, current loss 0.3438.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:09<00:00,  1.03it/s]\n",
      "\u001b[32m2026-02-17 18:17:05.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_128_128/20260217-181705\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:05.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1062.36it/s]\n",
      "\u001b[32m2026-02-17 18:17:06.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5504 test 0.4575 metric ['0.8284']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1105.65it/s]\n",
      "\u001b[32m2026-02-17 18:17:07.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3846 test 0.3843 metric ['0.8630']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1131.03it/s]\n",
      "\u001b[32m2026-02-17 18:17:08.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3400 test 0.3991 metric ['0.8563']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:08.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3843, current loss 0.3991.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1172.01it/s]\n",
      "\u001b[32m2026-02-17 18:17:09.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3239 test 0.3521 metric ['0.8724']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1134.72it/s]\n",
      "\u001b[32m2026-02-17 18:17:10.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3020 test 0.3603 metric ['0.8712']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:10.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3521, current loss 0.3603.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 977.36it/s] \n",
      "\u001b[32m2026-02-17 18:17:11.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2835 test 0.3438 metric ['0.8779']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1122.85it/s]\n",
      "\u001b[32m2026-02-17 18:17:12.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2728 test 0.3259 metric ['0.8819']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 932.59it/s]\n",
      "\u001b[32m2026-02-17 18:17:13.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2605 test 0.3460 metric ['0.8789']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:13.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3259, current loss 0.3460.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 973.92it/s]\n",
      "\u001b[32m2026-02-17 18:17:14.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2538 test 0.3235 metric ['0.8834']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 895.32it/s]\n",
      "\u001b[32m2026-02-17 18:17:15.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2450 test 0.3528 metric ['0.8777']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:15.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3235, current loss 0.3528.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:09<00:00,  1.02it/s]\n",
      "\u001b[32m2026-02-17 18:17:15.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_128_64/20260217-181715\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:15.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 954.75it/s]\n",
      "\u001b[32m2026-02-17 18:17:16.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5684 test 0.4813 metric ['0.8292']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1059.49it/s]\n",
      "\u001b[32m2026-02-17 18:17:17.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3953 test 0.3941 metric ['0.8604']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 821.84it/s]\n",
      "\u001b[32m2026-02-17 18:17:18.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3481 test 0.3928 metric ['0.8583']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1048.91it/s]\n",
      "\u001b[32m2026-02-17 18:17:19.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3316 test 0.3700 metric ['0.8698']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1046.06it/s]\n",
      "\u001b[32m2026-02-17 18:17:20.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3079 test 0.3526 metric ['0.8724']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 893.79it/s]\n",
      "\u001b[32m2026-02-17 18:17:21.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2936 test 0.3762 metric ['0.8650']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:21.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3526, current loss 0.3762.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1070.31it/s]\n",
      "\u001b[32m2026-02-17 18:17:22.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2850 test 0.3522 metric ['0.8752']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1102.75it/s]\n",
      "\u001b[32m2026-02-17 18:17:23.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2666 test 0.3278 metric ['0.8841']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:01<00:00, 852.78it/s]\n",
      "\u001b[32m2026-02-17 18:17:24.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2623 test 0.3407 metric ['0.8742']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:24.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3278, current loss 0.3407.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1079.47it/s]\n",
      "\u001b[32m2026-02-17 18:17:25.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2493 test 0.3346 metric ['0.8849']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:25.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3278, current loss 0.3346.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:10<00:00,  1.03s/it]\n",
      "\u001b[32m2026-02-17 18:17:25.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_64_256/20260217-181725\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:25.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1066.09it/s]\n",
      "\u001b[32m2026-02-17 18:17:26.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5491 test 0.4474 metric ['0.8347']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1074.30it/s]\n",
      "\u001b[32m2026-02-17 18:17:27.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3862 test 0.3999 metric ['0.8546']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1101.54it/s]\n",
      "\u001b[32m2026-02-17 18:17:28.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3452 test 0.3864 metric ['0.8614']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1153.78it/s]\n",
      "\u001b[32m2026-02-17 18:17:29.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3230 test 0.3848 metric ['0.8601']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1082.06it/s]\n",
      "\u001b[32m2026-02-17 18:17:30.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3028 test 0.3680 metric ['0.8707']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1164.20it/s]\n",
      "\u001b[32m2026-02-17 18:17:31.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2910 test 0.3415 metric ['0.8775']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1046.52it/s]\n",
      "\u001b[32m2026-02-17 18:17:32.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2777 test 0.3757 metric ['0.8718']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:32.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3415, current loss 0.3757.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1108.58it/s]\n",
      "\u001b[32m2026-02-17 18:17:33.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2698 test 0.3478 metric ['0.8768']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:33.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3415, current loss 0.3478.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1109.42it/s]\n",
      "\u001b[32m2026-02-17 18:17:34.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2576 test 0.3200 metric ['0.8840']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 973.54it/s]\n",
      "\u001b[32m2026-02-17 18:17:35.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2470 test 0.3489 metric ['0.8817']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:35.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3200, current loss 0.3489.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:09<00:00,  1.06it/s]\n",
      "\u001b[32m2026-02-17 18:17:35.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_64_128/20260217-181735\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:35.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1220.63it/s]\n",
      "\u001b[32m2026-02-17 18:17:36.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5631 test 0.4396 metric ['0.8442']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1179.92it/s]\n",
      "\u001b[32m2026-02-17 18:17:36.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3927 test 0.4157 metric ['0.8522']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1021.48it/s]\n",
      "\u001b[32m2026-02-17 18:17:37.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3584 test 0.3862 metric ['0.8616']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1156.40it/s]\n",
      "\u001b[32m2026-02-17 18:17:38.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3312 test 0.3734 metric ['0.8667']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1220.70it/s]\n",
      "\u001b[32m2026-02-17 18:17:39.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3097 test 0.3642 metric ['0.8688']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1120.45it/s]\n",
      "\u001b[32m2026-02-17 18:17:40.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2991 test 0.3480 metric ['0.8753']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1140.41it/s]\n",
      "\u001b[32m2026-02-17 18:17:41.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2848 test 0.3504 metric ['0.8771']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:41.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3480, current loss 0.3504.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1172.03it/s]\n",
      "\u001b[32m2026-02-17 18:17:42.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2747 test 0.3392 metric ['0.8821']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1224.92it/s]\n",
      "\u001b[32m2026-02-17 18:17:43.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2660 test 0.3389 metric ['0.8803']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1192.93it/s]\n",
      "\u001b[32m2026-02-17 18:17:44.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2544 test 0.3360 metric ['0.8775']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:08<00:00,  1.13it/s]\n",
      "\u001b[32m2026-02-17 18:17:44.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs/units_64_64/20260217-181744\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:44.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1025.99it/s]\n",
      "\u001b[32m2026-02-17 18:17:45.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5929 test 0.4681 metric ['0.8338']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1168.92it/s]\n",
      "\u001b[32m2026-02-17 18:17:45.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.4067 test 0.4180 metric ['0.8461']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1171.01it/s]\n",
      "\u001b[32m2026-02-17 18:17:46.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3612 test 0.4071 metric ['0.8531']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1185.05it/s]\n",
      "\u001b[32m2026-02-17 18:17:47.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3425 test 0.3841 metric ['0.8627']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1146.75it/s]\n",
      "\u001b[32m2026-02-17 18:17:48.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3242 test 0.3756 metric ['0.8643']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1166.55it/s]\n",
      "\u001b[32m2026-02-17 18:17:49.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.3101 test 0.3630 metric ['0.8705']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1201.56it/s]\n",
      "\u001b[32m2026-02-17 18:17:50.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2996 test 0.3587 metric ['0.8693']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1059.60it/s]\n",
      "\u001b[32m2026-02-17 18:17:51.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2860 test 0.3449 metric ['0.8761']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1201.55it/s]\n",
      "\u001b[32m2026-02-17 18:17:52.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2779 test 0.3612 metric ['0.8712']\u001b[0m\n",
      "\u001b[32m2026-02-17 18:17:52.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3449, current loss 0.3612.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:00<00:00, 1203.06it/s]\n",
      "\u001b[32m2026-02-17 18:17:52.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2688 test 0.3383 metric ['0.8737']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:08<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "units = [256, 128, 64]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "\n",
    "        logdir = f\"modellogs/units_{unit1}_{unit2}\"\n",
    "        \n",
    "        settings = TrainerSettings(\n",
    "            epochs=10,\n",
    "            metrics=[accuracy],\n",
    "            logdir=logdir, \n",
    "            train_steps=len(train),\n",
    "            valid_steps=len(valid),\n",
    "            reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    "            earlystop_kwargs={\n",
    "                \"patience\": 5\n",
    "            }\n",
    "        )\n",
    "\n",
    "        model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "        )\n",
    "        trainer.loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/stevenbontius/Documents/Projects/MADS/MADS-ML-StevenB/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "network_configs = [\n",
    "    {\"units1\": 64, \"units2\": 32},\n",
    "    {\"units1\": 128, \"units2\": 64},\n",
    "    {\"units1\": 256, \"units2\": 128},\n",
    "    {\"units1\": 512, \"units2\": 256},\n",
    "]\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have set the ReportType to TOML, you will find in every log dir a model.toml and settings.toml file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment, and study the result with tensorboard. \n",
    "\n",
    "Locally, it is easy to do that with VS code itself. On the server, you have to take these steps:\n",
    "\n",
    "- in the terminal, `cd` to the location of the repository\n",
    "- activate the python environment for the shell. Note how the correct environment is being activated.\n",
    "- run `tensorboard --logdir=modellogs` in the terminal\n",
    "- tensorboard will launch at `localhost:6006` and vscode will notify you that the port is forwarded\n",
    "- you can either press the `launch` button in VScode or open your local browser at `localhost:6006`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Report\n",
    "## 1. experiment\n",
    "Experiment with things like:\n",
    "- change the number of epochs, eg to 5 or 10. \n",
    "- changing the amount of units1 and units2 to values between 16 and 1024. Use factors of 2 to easily scan the ranges: 16, 32, 64, etc.\n",
    "- changing the batchsize to values between 4 and 128. Again, use factors of two for convenience.\n",
    "- change the depth of your model by adding a additional linear layer + activation function\n",
    "- changing the learningrate to values between 1e-2 and 1e-5 \n",
    "- changing the optimizer from SGD to one of the other available algoritms at [torch](https://pytorch.org/docs/stable/optim.html) (scroll down for the algorithms)\n",
    "\n",
    "Check the results:\n",
    "- all your experiments are saved in the `modellogs` directory, with a timestamp. Inside you find a model.toml file, that \n",
    "contains all the settings of the model. The `events` file is what tensorboard will show.\n",
    "- visualize the relationship between variables: for example, make a heatmap of units vs layers.\n",
    "\n",
    "Studyquestions:\n",
    "- Epochs: what is the upside, what is the downside of increasing epochs? Do you need more epochs to find out which configuration is best? When do you need that, when not?\n",
    "- what is an upside of using factors of 2 for hypertuning? What is a downside?\n",
    "\n",
    "## Note\n",
    "A note on train_steps: this is a setting that determines how often you get an update. \n",
    "Because our complete dataset is 938 (60000 / 64) batches long, you will need 938 trainstep to cover the complete 60.000 images.\n",
    "\n",
    "This can actually be a bit confusion for some students, because changing the value of trainsteps 938 changes the meaning of an `epoch` slightly, because one epoch is no longer the full dataset, but simply `trainstep` batches. Setting trainsteps to 100 means you need to wait twice as long before you get feedback on the performance, as compared to trainsteps=50. You will see that settings trainsteps to 100 improves the learning, but that is simply because the model has seen twice as much examples as compared to trainsteps=50.\n",
    "\n",
    "This implies that it is not usefull to compare trainsteps=50 and trainsteps=100, because setting it to 100 will always be better.\n",
    "Just pick an amount that works for your hardware & patience, and adjust your number of epochs accordingly (increase epochs with lower values for trainsteps)\n",
    "\n",
    "# 2. Reflect\n",
    "Doing a master means you don't just start engineering a pipeline, but you need to reflect. Why do you see the results you see? What does this mean, considering the theory? Write down lessons learned and reflections, based on experimental results. This is the `science` part of `data science`.\n",
    "\n",
    "You follow this cycle:\n",
    "- make a hypothesis\n",
    "- design an experiment\n",
    "- run the experiment\n",
    "- analyze the results and draw conclusions\n",
    "- repeat\n",
    "\n",
    "## Tip\n",
    "To keep track of this process, it is useful to keep a journal. While you could use anything to do so, a nice command line tool is [jrnl](https://jrnl.sh/en/stable/). This gives you the advantage of staying in the terminal, just type down your ideas during the process, and you can always look back at what you have done.\n",
    "Try to first formulate a hypothesis, and then design an experiment to test it. This will help you to stay focused on the goal, and not get lost in the data.\n",
    "\n",
    "Important: the report you write is NOT the same as your journal! The journal will help you to keep track of your process, and later write down a reflection on what you have done where you draw conclusion, reflecting back on the theory.\n",
    "\n",
    "# 3. Make a short report\n",
    "Make a short 1 a4 page report of your findings.\n",
    "pay attention to:\n",
    "- what was your hypothesis about interaction between hyperparameters?\n",
    "- what did you find?\n",
    "- visualise your results about the relationship between hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
